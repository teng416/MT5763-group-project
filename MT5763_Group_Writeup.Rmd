---
title: "MT5763 Group Project"
author: "Group Yojimbo"
output:
  pdf_document: default
  html_document: default
---
The aim of this project is to produce a quick, efficient function in R capable of producing linear regression coefficients for a data set using bootstrapping. The approach used for this project is vectorisation of the code, in lieu of using slow loops and inbuilt R functions.

##Underlying Mathematics

The new function, "boot.lm.vector" uses a vectorised form of linear regression to produce the coefficients. The input data must have its columns ordered as follows: "Intercept", "Covariates", "Dependent Variable". For the column "Intercept", the following results were achieved.

Intercept | x1 | x2 | ... | xn  | y
----------|----|----|-----|-----|----
1         | x11| x12| ... | x1n | y1

**Importantly, data must be a matrix rather than a dataframe, as this simplifies mathematical operations.** We shall now discuss the mathematics behind how this function operates. The matrix X is construncted, which includes the interception and covariates:

$$
\textbf{X} = \begin{pmatrix} 1 & x_{11} & x_{12}  & \cdots & x_{1n} \\
1 & x_{21} & x_{22}  & \cdots & x_{2n} \\
\vdots & \vdots & \vdots & \ddots & \vdots & \\
1 & x_{n1} & x_{n2} & \cdots & x_{nn}
\end{pmatrix}
$$
and the vector y that includes the dependent variable:

$$
y = \begin{pmatrix} y_{1} \\ y _{2} \\ \vdots \\ y_{n} \end{pmatrix}
$$
We wish to solve the matrix equation
$$
\textbf{X} z = y
$$
Where $z$ is a vector containing our regression coefficients, ie if we compute the first line of this calculation we find:

$$
z_{1} + z_{2}x_{11} + z_{3}x_{12} + \cdots + z_{n}x_{1n} = y_{1}
$$
Solving over the entire dataset gives us a good idea of what the values of $z_{i}$ are. R has the command solve() built in for exactly this type of problem, however solve() only works if $\textbf{X}$ is a square matrix. To remedy this, we multiply both sides of the equation by the transpose of $\textbf{X}$. This is what is being done using crossprod(). This obviously has no effect on the solutions of $z_{i}$, it merely frames the problem in such a way that solve() is can be used.

##Implementing the Code

Having covered the mathematical framework, we now look at the code that actually does this.

```{r}

#Generating some random data to work with.
set.seed(1)
n <- 100
x <- runif(n)
y <- x + 1 + rexp(n)


#Data for use with vector needs to be ordered: Intercept, covariates, dependent variable.
data.m <- cbind("(Intercept)" = 1, x, y)
data.f <- data.frame(x,y)


#~~~~~~~~~~Vectorisation~~~~~~~~~~~~~~~~~~~~~~~~~

boot.lm.vector <- function(index, inputData) {
  set.seed(index)
  #Random sampling from input data with replacement.
  d <- inputData[sample.int(nrow(inputData), replace = T),]
  
  #Define matrix with covariates and intercept.
  a <- ncol(inputData)-1
  X <- d[, 1:a]
  
  #Vector with dependent variable.
  y <- d[, a+1]
  
  #Solve for coefficients. Solve requires square matrix, hence use of crossprod
  solve(crossprod(X), crossprod(X,y))
}

#Non parallel method of repeating bootstraps, still produces very respectable times.

system.time(r1 <- t(lapply(1:10000, boot.lm.vector, inputData = data.m)))

r1df <- plyr::ldply(r1)
mylist <- r1df[,1]
resultsHolder1 <- numeric(ncol(data.m)-1)
for(i in 1:ncol(data.m)-1){
  resultsHolder1[i] <- mean(mylist[seq(i,nrow(r1df),ncol(data.m)-1)])
}
resultsHolder1

```

```{r}
#~~~~~~~~~~~~~~~~Parallelising~~~~~~~~~~~~~~~~~~~~~

library(parallel)
library(doParallel)
nCores <- detectCores()
myClust <- makeCluster(nCores-1, type = "PSOCK")


parallelTime <- system.time(rtest <- parLapply(myClust, 1:10000, fun = boot.lm.vector, inputData = data.m)) 
parallelTime

rtestdf <- plyr::ldply(rtest)
mylist <- rtestdf[,1]
resultsHolder <- numeric(ncol(data.m)-1)
for(i in 1:ncol(data.m)-1){
  resultsHolder[i] <- mean(mylist[seq(i,nrow(rtestdf),ncol(data.m)-1)])
}
resultsHolder


```

We wish to visualise how well our function has fit the data. 


![**Figure 1: Comparison of trendlines generated by our function and lm()**](https://raw.githubusercontent.com/teng416/MT5763-group-project/master/figures/plots%20with%20trenlines.png)

It is clear to see that lines are right on top of each other - our function produces results at least as good as lm().

###Summary of how to use the function

1. Place the dependent variable is in the last column of the data.
2. Add an 'intercept' column in the first column.
3. Ensure that the data is in matrix form rather than a dataframe.


##Timing 

We have established that the function works, so how much quicker is it than the original?

```{r}
lmBoot <- function(inputData, nBoot){
  
  for(i in 1:nBoot){
    
    # resample our data with replacement
    bootData <- inputData[sample(1:nrow(inputData), nrow(inputData), replace = T),]
    
    # fit the model under this alternative reality
    bootLM <- lm(y ~ x, data = bootData)
    
    # store the coefs
    if(i == 1){
      
      bootResults <- matrix(coef(bootLM), ncol = 2)
      
    } else {
      
      bootResults<- rbind(bootResults, matrix(coef(bootLM), ncol = 2))
      
    }
    
    
  } # end of i loop
  
  bootResults
  
}

CarlTime <- system.time(r2 <- lmBoot(data.f, 10000))
CarlTime

CarlResults <- c(mean(r2[,1]), mean(r2[,2]))
CarlResults
```
This is dramatically slower than our improved function! We have clearly made significant improvement.
A further question arises; how do we compare with packages designed for bootstrapping?

```{r}

library(microbenchmark)
library(boot)

#Defining a simple function that will work with boot package.
bootpkg.lm <- function(formula, data, indices){
  d <- data[indices, ]
  fit <- lm(formula, data = d)
  return(coef(fit))
}

#system.time(results <- boot(data = data.f, statistic = bootpkg.lm, R=10000, formula = y ~ x))
#results

#Now performing the microbenchmarking. Only using 1000 bootstraps and 10 repititions due to how long
#the boot() expression takes (I did wait it out once, results were similar for 100 repitions).
microbenchmark(parLapply(myClust, 1:1000, fun = boot.lm.vector, inputData = data.m), times = 10)
microbenchmark(boot(data = data.f, statistic = bootpkg.lm, R=1000, formula = y ~.), times = 10)
```

Again, we are much quicker!

##Conclusion

To conclude, we can see that we obtain effectively the same results as the original function. Any discrepancies (although small) will be due to the unavoidably random nature of bootstrapping. We are also significantly faster than the original function.

```{r}
resultsHolder
CarlResults

parallelTime
CarlTime
```


To get a closer look at why our function is faster, we examine the profile of the original function.

![**Figure 2: Original function profile**](https://raw.githubusercontent.com/teng416/MT5763-group-project/master/original_fun_profile_pic.PNG)

We can see that the use of the lm and rbind functions eats up the vast majority of the total function runtime. Our approach, which bypasses the use of these functions is much faster as a result.


#SAS function

##Original macro
The original macro uses loops to generate simulated datasets and calculate coefficents. For each loop, a new simulated dataset is generated, coeffcients are calculated and results are stored in ResultHolder. As SAS needs to run regression procedure for each loop and log the status, it will be slow. 

##Improved macro:
The improved macro did not use loops, and produced the simulated datasets all at once in a new dataset and stored them as long format. The coefficients were then calculated using regression procedure according to index. For detailed SAS code, please refer to SAS file in the repository.

##Time comparison:
The timer function records the time at which the macro starts running and the time the macro stops running. The time is calculated from the difference between these. 

Original Macro: for 100 iterations, uses 12 seconds on average.
Improved Macro: for 100 iterations, uses 0.12 seconds on average.

This is a significant improvement!

##RTF file:
The improved Macro will output an RTF file containing mean for each coefficients, 95% confidence intervals for each coefficient and plot of coefficients distribution as required using ODS. 

##Using the function
The function operates on exactly the same input variables as the original function. The only difference (apart from the increased speed) is that our results are outputted to an rtf file and contain extra information, such as means, confidence intervals and plots. 


